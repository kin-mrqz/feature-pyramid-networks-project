{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfe5c9e",
   "metadata": {},
   "source": [
    "# FPN Object Detection Visualization\n",
    "\n",
    "This notebook demonstrates the Feature Pyramid Network (FPN) bounding box detection mechanism. We'll load a trained model, perform inference on images, and visualize the detected objects with bounding boxes, class labels, and confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e9f72",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a1828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add modernized_fpn directory to path to import modules\n",
    "sys.path.insert(0, os.path.abspath('modernized_fpn'))\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from model import Model\n",
    "from backbone.base import Base as BackboneBase\n",
    "from dataset.base import Base as DatasetBase\n",
    "from config.eval_config import EvalConfig as Config\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0918",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c507f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\COMP3340\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\COMP3340\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DatasetBase\u001b[38;5;241m.\u001b[39mfrom_name(DATASET_NAME)\n\u001b[0;32m     16\u001b[0m backbone \u001b[38;5;241m=\u001b[39m BackboneBase\u001b[38;5;241m.\u001b[39mfrom_name(BACKBONE_NAME)(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooling_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOOLING_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mANCHOR_RATIOS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mANCHOR_SCALES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrpn_pre_nms_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRPN_PRE_NMS_TOP_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrpn_post_nms_top_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRPN_POST_NMS_TOP_N\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m     28\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(CHECKPOINT_PATH)\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive\\Uni\\Y3 Sem1\\COMP3314\\Group_Project\\feature-pyramid-networks-project\\modernized_fpn\\model.py:65\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, backbone, num_classes, pooling_mode, anchor_ratios, anchor_scales, rpn_pre_nms_top_n, rpn_post_nms_top_n)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn \u001b[38;5;241m=\u001b[39m RegionProposalNetwork(num_features_out, anchor_ratios, anchor_scales, rpn_pre_nms_top_n, rpn_post_nms_top_n)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetection \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooling_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\OneDrive\\Uni\\Y3 Sem1\\COMP3314\\Group_Project\\feature-pyramid-networks-project\\modernized_fpn\\model.py:177\u001b[0m, in \u001b[0;36mModel.Detection.__init__\u001b[1;34m(self, pooling_mode, num_classes)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1024\u001b[39m, num_classes)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1024\u001b[39m, num_classes \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_normalize_mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_normalize_std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.2\u001b[39m, \u001b[38;5;241m.2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\COMP3340\\lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CHECKPOINT_PATH = 'modernized_fpn/outputs/checkpoints-20251119144440-voc2007-resnet101-98a73035/model-80000.pth'\n",
    "DATASET_NAME = 'voc2007'\n",
    "BACKBONE_NAME = 'resnet101'\n",
    "\n",
    "# VOC 2007 class names\n",
    "VOC_CLASSES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
    "    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "dataset = DatasetBase.from_name(DATASET_NAME)\n",
    "backbone = BackboneBase.from_name(BACKBONE_NAME)(pretrained=False)\n",
    "model = Model(\n",
    "    backbone, \n",
    "    dataset.num_classes(), \n",
    "    pooling_mode=Config.POOLING_MODE,\n",
    "    anchor_ratios=Config.ANCHOR_RATIOS, \n",
    "    anchor_scales=Config.ANCHOR_SCALES,\n",
    "    rpn_pre_nms_top_n=Config.RPN_PRE_NMS_TOP_N, \n",
    "    rpn_post_nms_top_n=Config.RPN_POST_NMS_TOP_N\n",
    ").cuda()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from step {checkpoint['step']}\")\n",
    "print(f\"Number of classes: {len(VOC_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d12ad",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, min_side=800, max_side=1333):\n",
    "    \"\"\"Load and preprocess image for the model\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_image = image.copy()\n",
    "    \n",
    "    # Resize image\n",
    "    w, h = image.size\n",
    "    scale = min(min_side / min(h, w), max_side / max(h, w))\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    image = image.resize((new_w, new_h), Image.BILINEAR)\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(image).cuda()\n",
    "    \n",
    "    return image_tensor, original_image, scale\n",
    "\n",
    "\n",
    "def run_inference(model, image_tensor, confidence_threshold=0.7):\n",
    "    \"\"\"Run model inference and return detections\"\"\"\n",
    "    with torch.no_grad():\n",
    "        forward_input = Model.ForwardInput.Eval(image_tensor)\n",
    "        forward_output = model.eval().forward(forward_input)\n",
    "    \n",
    "    detection_bboxes = forward_output.detection_bboxes\n",
    "    detection_classes = forward_output.detection_classes\n",
    "    detection_probs = forward_output.detection_probs\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    mask = detection_probs >= confidence_threshold\n",
    "    detection_bboxes = detection_bboxes[mask]\n",
    "    detection_classes = detection_classes[mask]\n",
    "    detection_probs = detection_probs[mask]\n",
    "    \n",
    "    return detection_bboxes, detection_classes, detection_probs\n",
    "\n",
    "\n",
    "def visualize_detections(image, bboxes, classes, probs, scale, class_names=VOC_CLASSES):\n",
    "    \"\"\"Visualize detections with bounding boxes\"\"\"\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Color map for different classes\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for bbox, cls, prob in zip(bboxes, classes, probs):\n",
    "        # Scale bbox back to original image size\n",
    "        x1, y1, x2, y2 = bbox / scale\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Get class info\n",
    "        class_idx = cls.item()\n",
    "        class_name = class_names[class_idx]\n",
    "        color = colors[class_idx]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        label = f'{class_name}: {prob:.2f}'\n",
    "        ax.text(\n",
    "            x1, y1 - 5, label,\n",
    "            bbox=dict(facecolor=color, alpha=0.7),\n",
    "            fontsize=10, color='white', weight='bold'\n",
    "        )\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def draw_detections_pil(image, bboxes, classes, probs, scale, class_names=VOC_CLASSES):\n",
    "    \"\"\"Draw detections on PIL image for saving\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Try to load a font, fallback to default\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Color map\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(class_names)))\n",
    "    \n",
    "    for bbox, cls, prob in zip(bboxes, classes, probs):\n",
    "        # Scale bbox back to original image size\n",
    "        x1, y1, x2, y2 = bbox / scale\n",
    "        \n",
    "        # Get class info\n",
    "        class_idx = cls.item()\n",
    "        class_name = class_names[class_idx]\n",
    "        color_rgb = tuple((np.array(colors[class_idx][:3]) * 255).astype(int))\n",
    "        \n",
    "        # Draw rectangle\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color_rgb, width=3)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f'{class_name}: {prob:.2f}'\n",
    "        text_bbox = draw.textbbox((x1, y1 - 20), label, font=font)\n",
    "        draw.rectangle(text_bbox, fill=color_rgb)\n",
    "        draw.text((x1, y1 - 20), label, fill='white', font=font)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef21e9",
   "metadata": {},
   "source": [
    "## 4. Run Detection on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your test image path (update this to your actual image path)\n",
    "IMAGE_PATH = 'modernized_fpn/data/VOCdevkit/VOC2007/JPEGImages/000001.jpg'\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Process image\n",
    "print(\"Processing image...\")\n",
    "image_tensor, original_image, scale = preprocess_image(IMAGE_PATH)\n",
    "print(f\"Image size: {original_image.size}\")\n",
    "print(f\"Scaled size: {image_tensor.shape}\")\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference...\")\n",
    "detection_bboxes, detection_classes, detection_probs = run_inference(\n",
    "    model, image_tensor, CONFIDENCE_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"\\nDetections: {len(detection_bboxes)} objects found\")\n",
    "for bbox, cls, prob in zip(detection_bboxes, detection_classes, detection_probs):\n",
    "    print(f\"  - {VOC_CLASSES[cls.item()]}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445c286",
   "metadata": {},
   "source": [
    "## 5. Visualize Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detections\n",
    "fig = visualize_detections(\n",
    "    original_image, \n",
    "    detection_bboxes, \n",
    "    detection_classes, \n",
    "    detection_probs, \n",
    "    scale\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88234f75",
   "metadata": {},
   "source": [
    "## 6. Compare Different Confidence Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe806bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different confidence thresholds\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, threshold in enumerate(thresholds):\n",
    "    # Run inference with different thresholds\n",
    "    bboxes, classes, probs = run_inference(model, image_tensor, threshold)\n",
    "    \n",
    "    # Display on subplot\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(original_image)\n",
    "    ax.set_title(f'Confidence â‰¥ {threshold} ({len(bboxes)} detections)', fontsize=14, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(VOC_CLASSES)))\n",
    "    \n",
    "    for bbox, cls, prob in zip(bboxes, classes, probs):\n",
    "        x1, y1, x2, y2 = bbox / scale\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        class_idx = cls.item()\n",
    "        color = colors[class_idx]\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        label = f'{VOC_CLASSES[class_idx]}: {prob:.2f}'\n",
    "        ax.text(\n",
    "            x1, y1 - 5, label,\n",
    "            bbox=dict(facecolor=color, alpha=0.7),\n",
    "            fontsize=8, color='white', weight='bold'\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2efb1b",
   "metadata": {},
   "source": [
    "## 7. Analyze Detections by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze detections\n",
    "bboxes, classes, probs = run_inference(model, image_tensor, 0.5)\n",
    "\n",
    "# Count detections per class\n",
    "class_counts = {}\n",
    "class_confidences = {}\n",
    "\n",
    "for cls, prob in zip(classes, probs):\n",
    "    class_name = VOC_CLASSES[cls.item()]\n",
    "    if class_name not in class_counts:\n",
    "        class_counts[class_name] = 0\n",
    "        class_confidences[class_name] = []\n",
    "    class_counts[class_name] += 1\n",
    "    class_confidences[class_name].append(prob.item())\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Bar chart of detections per class\n",
    "if class_counts:\n",
    "    classes_list = list(class_counts.keys())\n",
    "    counts_list = list(class_counts.values())\n",
    "    \n",
    "    ax1.bar(range(len(classes_list)), counts_list, color='steelblue')\n",
    "    ax1.set_xticks(range(len(classes_list)))\n",
    "    ax1.set_xticklabels(classes_list, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Number of Detections')\n",
    "    ax1.set_title('Detections per Class', fontsize=14, weight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Box plot of confidence scores\n",
    "    confidence_data = [class_confidences[c] for c in classes_list]\n",
    "    bp = ax2.boxplot(confidence_data, labels=classes_list, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    ax2.set_xticklabels(classes_list, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Confidence Score')\n",
    "    ax2.set_title('Confidence Distribution per Class', fontsize=14, weight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No detections found', ha='center', va='center', fontsize=14)\n",
    "    ax2.text(0.5, 0.5, 'No detections found', ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nDetection Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for class_name, count in class_counts.items():\n",
    "    avg_conf = np.mean(class_confidences[class_name])\n",
    "    print(f\"{class_name:15s}: {count:2d} objects (avg confidence: {avg_conf:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73daf7f5",
   "metadata": {},
   "source": [
    "## 8. Batch Processing Multiple Images\n",
    "\n",
    "Process multiple images from the dataset to showcase the bounding box detection mechanism across various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d855310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get multiple test images\n",
    "image_dir = 'modernized_fpn/data/VOCdevkit/VOC2007/JPEGImages/'\n",
    "image_files = glob.glob(os.path.join(image_dir, '*.jpg'))[:6]  # Process first 6 images\n",
    "\n",
    "# Create grid visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, image_path in enumerate(image_files):\n",
    "    # Process image\n",
    "    image_tensor, original_image, scale = preprocess_image(image_path)\n",
    "    bboxes, classes, probs = run_inference(model, image_tensor, 0.5)\n",
    "    \n",
    "    # Display\n",
    "    ax = axes[idx]\n",
    "    ax.imshow(original_image)\n",
    "    ax.set_title(f'{os.path.basename(image_path)}\\n{len(bboxes)} detections', fontsize=10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(VOC_CLASSES)))\n",
    "    \n",
    "    for bbox, cls, prob in zip(bboxes, classes, probs):\n",
    "        x1, y1, x2, y2 = bbox / scale\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        class_idx = cls.item()\n",
    "        color = colors[class_idx]\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        label = f'{VOC_CLASSES[class_idx]}: {prob:.2f}'\n",
    "        ax.text(\n",
    "            x1, y1 - 3, label,\n",
    "            bbox=dict(facecolor=color, alpha=0.7),\n",
    "            fontsize=7, color='white', weight='bold'\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6a723",
   "metadata": {},
   "source": [
    "## 9. Save Detection Results\n",
    "\n",
    "Save the visualization with bounding boxes to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c046a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save a detection result\n",
    "output_path = 'detection_result.jpg'\n",
    "\n",
    "# Load and process image\n",
    "image_tensor, original_image, scale = preprocess_image(IMAGE_PATH)\n",
    "bboxes, classes, probs = run_inference(model, image_tensor, 0.5)\n",
    "\n",
    "# Draw on PIL image\n",
    "result_image = draw_detections_pil(\n",
    "    original_image.copy(), \n",
    "    bboxes, \n",
    "    classes, \n",
    "    probs, \n",
    "    scale\n",
    ")\n",
    "\n",
    "# Save\n",
    "result_image.save(output_path, quality=95)\n",
    "print(f\"Saved detection result to: {output_path}\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.title('Saved Detection Result', fontsize=14, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP3340",
   "language": "python",
   "name": "comp3340"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
